{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Searching for the Higgs Boson #\n\nThe Standard Model is a theory in particle physics that describes some of the most basic forces of nature. One fundamental particle, the Higgs boson, is what accounts for the *mass* of matter. First theorized in the 1964, the Higgs boson eluded observation for almost fifty years. In 2012 it was finally observed experimentally at the Large Hadron Collider. These experiments produced millions of gigabytes of data.\n\nLarge and complicated datasets like these are where deep learning excels. In this notebook, we'll build a Wide and Deep neural network to determine whether an observed particle collision produced a Higgs boson or not.\n\n# The Collision Data #\n\nThe collision of protons at high energy can produce new particles like the Higgs boson. These particles can't be directly observed, however, since they decay almost instantly. So to detect the presence of a new particle, we instead observe the behavior of the particles they decay into, their \"decay products\".\n\nThe *Higgs* dataset contains 21 \"low-level\" features of the decay products and also 7 more \"high-level\" features derived from these.\n\n# Wide and Deep Neural Networks #\n\nA *Wide and Deep* network trains a linear layer side-by-side with a deep stack of dense layers. Wide and Deep networks are often effective on tabular datasets.[^1]\n\nBoth the dataset and the model are much larger than what we used in the course. To speed up training, we'll use Kaggle's [Tensor Processing Units](https://www.kaggle.com/docs/tpu) (TPUs), an accelerator ideal for large workloads.\n\nWe've collected some hyperparameters here to make experimentation easier. Fork this notebook by [**clicking here**](https://www.kaggle.com/kernels/fork/12171965) to try it yourself!","metadata":{}},{"cell_type":"code","source":"# Model Configuration\nUNITS = 2 ** 11 # 2048\nACTIVATION = 'relu'\nDROPOUT = 0.1\n\n# Training Configuration\nBATCH_SIZE_PER_REPLICA = 2 ** 11 # powers of 128 are best","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:12:19.244893Z","iopub.execute_input":"2023-10-25T07:12:19.245197Z","iopub.status.idle":"2023-10-25T07:12:19.249319Z","shell.execute_reply.started":"2023-10-25T07:12:19.245174Z","shell.execute_reply":"2023-10-25T07:12:19.248663Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"The next few sections set up the TPU computation, data pipeline, and neural network model. If you'd just like to see the results, feel free to skip to the end!\n\n# Setup #\n\nIn addition to our imports, this section contains some code that will connect our notebook to the TPU and create a **distribution strategy**. Each TPU has eight computational cores acting independently. With a distribution strategy, we define how we want to divide up the work between them.","metadata":{}},{"cell_type":"code","source":"# TensorFlow\nimport tensorflow as tf\nprint(\"Tensorflow version \" + tf.__version__)\n\n# Detect and init the TPU\ntry: # detect TPUs\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError: # detect GPUs\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n    \n# Plotting\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Matplotlib defaults\nplt.style.use('seaborn-whitegrid')\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\n\n\n# Data\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.io import FixedLenFeature\nAUTO = tf.data.experimental.AUTOTUNE\n\n\n# Model\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:12:23.819529Z","iopub.execute_input":"2023-10-25T07:12:23.819883Z","iopub.status.idle":"2023-10-25T07:13:13.872507Z","shell.execute_reply.started":"2023-10-25T07:12:23.819858Z","shell.execute_reply":"2023-10-25T07:13:13.871276Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"D1025 07:12:55.172944907      15 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\nD1025 07:12:55.172974841      15 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\nD1025 07:12:55.172978020      15 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\nD1025 07:12:55.172980368      15 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\nD1025 07:12:55.172982580      15 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\nD1025 07:12:55.172985016      15 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\nD1025 07:12:55.172987238      15 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\nD1025 07:12:55.172990095      15 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\nD1025 07:12:55.172992195      15 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\nD1025 07:12:55.172994306      15 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\nD1025 07:12:55.172996289      15 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\nD1025 07:12:55.172998403      15 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\nD1025 07:12:55.173000495      15 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\nD1025 07:12:55.173002509      15 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\nI1025 07:12:55.173231395      15 ev_epoll1_linux.cc:122]               grpc epoll fd: 61\nD1025 07:12:55.173242880      15 ev_posix.cc:144]                      Using polling engine: epoll1\nD1025 07:12:55.173320266      15 dns_resolver_ares.cc:822]             Using ares dns resolver\nD1025 07:12:55.183723435      15 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD1025 07:12:55.183736802      15 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD1025 07:12:55.183740319      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD1025 07:12:55.183743003      15 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD1025 07:12:55.183745746      15 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD1025 07:12:55.183748438      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\nD1025 07:12:55.183755963      15 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD1025 07:12:55.183775644      15 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD1025 07:12:55.183805631      15 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD1025 07:12:55.183820461      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD1025 07:12:55.183823411      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD1025 07:12:55.183826282      15 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD1025 07:12:55.183833279      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\nD1025 07:12:55.183836147      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD1025 07:12:55.183838967      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD1025 07:12:55.183842669      15 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\nI1025 07:12:55.187334649      15 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\nI1025 07:12:55.203630841     242 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE1025 07:12:55.210181856     242 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2023-10-25T07:12:55.210165051+00:00\", grpc_status:2}\n","output_type":"stream"},{"name":"stdout","text":"Tensorflow version 2.12.0\nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\nINFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\nNumber of accelerators:  8\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_15/23444675.py:18: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n  plt.style.use('seaborn-whitegrid')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Notice that TensorFlow now detects eight accelerators. Using a TPU is a bit like using eight GPUs at once.\n\n# Load Data #\n\nThe dataset has been encoded in a binary file format called *TFRecords*. These two functions will parse the TFRecords and build a TensorFlow `tf.data.Dataset` object that we can use for training.","metadata":{}},{"cell_type":"code","source":"def make_decoder(feature_description):\n    def decoder(example):\n        example = tf.io.parse_single_example(example, feature_description)\n        features = tf.io.parse_tensor(example['features'], tf.float32)\n        features = tf.reshape(features, [28])\n        label = example['label']\n        return features, label\n    return decoder\n\ndef load_dataset(filenames, decoder, ordered=False):\n    AUTO = tf.data.experimental.AUTOTUNE\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n    dataset = (\n        tf.data\n        .TFRecordDataset(filenames, num_parallel_reads=AUTO)\n        .with_options(ignore_order)\n        .map(decoder, AUTO)\n    )\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:13:19.692386Z","iopub.execute_input":"2023-10-25T07:13:19.692812Z","iopub.status.idle":"2023-10-25T07:13:19.699313Z","shell.execute_reply.started":"2023-10-25T07:13:19.692781Z","shell.execute_reply":"2023-10-25T07:13:19.698455Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"dataset_size = int(11e6)\nvalidation_size = int(5e5)\ntraining_size = dataset_size - validation_size\n\n# For model.fit\nbatch_size = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\nsteps_per_epoch = training_size // batch_size\nvalidation_steps = validation_size // batch_size\n\n# For model.compile\nsteps_per_execution = 256","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:13:23.476209Z","iopub.execute_input":"2023-10-25T07:13:23.476566Z","iopub.status.idle":"2023-10-25T07:13:23.481258Z","shell.execute_reply.started":"2023-10-25T07:13:23.476540Z","shell.execute_reply":"2023-10-25T07:13:23.480443Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"feature_description = {\n    'features': FixedLenFeature([], tf.string),\n    'label': FixedLenFeature([], tf.float32),\n}\ndecoder = make_decoder(feature_description)\n\ndata_dir = KaggleDatasets().get_gcs_path('higgs-boson')\ntrain_files = tf.io.gfile.glob(data_dir + '/training' + '/*.tfrecord')\nvalid_files = tf.io.gfile.glob(data_dir + '/validation' + '/*.tfrecord')\n\nds_train = load_dataset(train_files, decoder, ordered=False)\nds_train = (\n    ds_train\n    .cache()\n    .repeat()\n    .shuffle(2 ** 19)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nds_valid = load_dataset(valid_files, decoder, ordered=False)\nds_valid = (\n    ds_valid\n    .batch(batch_size)\n    .cache()\n    .prefetch(AUTO)\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:13:26.185418Z","iopub.execute_input":"2023-10-25T07:13:26.185756Z","iopub.status.idle":"2023-10-25T07:13:26.330504Z","shell.execute_reply.started":"2023-10-25T07:13:26.185731Z","shell.execute_reply":"2023-10-25T07:13:26.329690Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"get_gcs_path is not required on TPU VMs which can directly use Kaggle datasets, using path: /kaggle/input/higgs-boson\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model #\n\nNow that the data is ready, let's define the network. We're defining the deep branch of the network using Keras's *Functional API*, which is a bit more flexible that the `Sequential` method we used in the course.\n","metadata":{}},{"cell_type":"code","source":"def dense_block(units, activation, dropout_rate, l1=None, l2=None):\n    def make(inputs):\n        x = layers.Dense(units)(inputs)\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation(activation)(x)\n        x = layers.Dropout(dropout_rate)(x)\n        return x\n    return make\n\nwith strategy.scope():\n    # Wide Network\n    wide = keras.experimental.LinearModel()\n\n    # Deep Network\n    inputs = keras.Input(shape=[28])\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(inputs)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    outputs = layers.Dense(1)(x)\n    deep = keras.Model(inputs=inputs, outputs=outputs)\n    \n    # Wide and Deep Network\n    wide_and_deep = keras.experimental.WideDeepModel(\n        linear_model=wide,\n        dnn_model=deep,\n        activation='sigmoid',\n    )\n\nwide_and_deep.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['AUC', 'binary_accuracy'],\n    experimental_steps_per_execution=steps_per_execution,\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:13:32.577590Z","iopub.execute_input":"2023-10-25T07:13:32.577953Z","iopub.status.idle":"2023-10-25T07:13:34.294545Z","shell.execute_reply.started":"2023-10-25T07:13:32.577928Z","shell.execute_reply":"2023-10-25T07:13:34.293601Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"WARNING:tensorflow:The argument `steps_per_execution` is no longer experimental. Pass `steps_per_execution` instead of `experimental_steps_per_execution`.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training #\n\nDuring training, we'll use the `EarlyStopping` callback as usual. Notice that we've also defined a **learning rate schedule**. It's been found that gradually decreasing the learning rate over the course of training can improve performance (the weights \"settle in\" to a minimum). This schedule will multiply the learning rate by `0.2` if the validation loss didn't decrease after an epoch.","metadata":{}},{"cell_type":"code","source":"early_stopping = callbacks.EarlyStopping(\n    patience=2,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nlr_schedule = callbacks.ReduceLROnPlateau(\n    patience=0,\n    factor=0.2,\n    min_lr=0.001,\n)","metadata":{"lines_to_next_cell":2,"execution":{"iopub.status.busy":"2023-10-25T07:13:39.046736Z","iopub.execute_input":"2023-10-25T07:13:39.047065Z","iopub.status.idle":"2023-10-25T07:13:39.051510Z","shell.execute_reply.started":"2023-10-25T07:13:39.047041Z","shell.execute_reply":"2023-10-25T07:13:39.050648Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"history = wide_and_deep.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=50,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n    callbacks=[early_stopping, lr_schedule],\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T07:13:41.015918Z","iopub.execute_input":"2023-10-25T07:13:41.016256Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/50\n256/640 [===========>..................] - ETA: 3:36 - loss: 0.6398 - auc: 0.7279 - binary_accuracy: 0.6695","output_type":"stream"}]},{"cell_type":"code","source":"history_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot(title='Cross-entropy Loss')\nhistory_frame.loc[:, ['auc', 'val_auc']].plot(title='AUC');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References #\n\n- Baldi, P. et al. *Searching for Exotic Particles in High-Energy Physics with Deep Learning*. (2014) ([arXiv](https://arxiv.org/abs/1402.4735))\n- Cheng, H. et al. *Wide & Deep Learning for Recommender Systems*. (2016) ([arXiv](https://arxiv.org/abs/1606.07792))\n- *What Exactly is the Higgs Boson?* Scientific American. (1999) [(article)](https://www.scientificamerican.com/article/what-exactly-is-the-higgs/)]\n\n[^1]: In the original implementation, categorical features were one-hot encoded and crossed to produce the interaction features. This \"wide\" dataset was used with the linear component. For the deep component, the categories were encoded into a much narrower embedding layer.","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/191966) to chat with other Learners.*","metadata":{}}]}